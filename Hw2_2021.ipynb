{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Get acquainted with different classification techniques.**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Victor Eberstein** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />\n",
    "\n",
    "---\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comments:\n",
    "* **Throughout this assignment, feel free to use any *NumPy*-functions where you see fit.**\n",
    "* **For the practical tasks there are comments marked with \"TODO\" in all parts that you need to change.**\n",
    "* **Please note the slight changes in the notation from the lectures:**\n",
    "    * Here, the dataset is represented as $\\{ (\\mathbf{x}_n, y_n)\\}_{n=1}^N$, where $x_n$ are vectors with attributes of dimension $D$ and $y_n$ is the associated labels ($y_n \\in \\{0,1\\}$ for binary prediction). In the lectures, $t_n$ was instead used for the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset\n",
    "**Run the code cell below to produce the dataset and some helper functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Randomly generate data from two different distributions\n",
    "data_class1, _ = make_gaussian_quantiles(mean = [1,1], cov = 1.5, n_samples = 30, n_features=2, random_state=18)\n",
    "data_class2, _ = make_gaussian_quantiles(mean = [-1,-1], cov= 0.75,n_samples = 8, n_features=2, random_state=4)\n",
    "\n",
    "# Concatenate the data, and add labels\n",
    "X_train = np.append(data_class1, data_class2, axis=0)\n",
    "y_train = np.append(np.zeros(len(data_class1), dtype=np.int), \n",
    "                   np.ones(len(data_class2), dtype=np.int))\n",
    "\n",
    "# Consider new test point\n",
    "X_test = np.array([[-0.6,-0.4]])\n",
    "\n",
    "def plot_data(X_train, y_train, X_test, title = ''):\n",
    "    # Plot the two data classes\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    ax.plot(X_train[y_train == 0][:,0], X_train[y_train == 0][:,1], 'o', markersize=8)\n",
    "    ax.plot(X_train[y_train == 1][:,0], X_train[y_train == 1][:,1], 's', markersize=8)\n",
    "\n",
    "    # Plot test point (circles to help with distances)\n",
    "    ax.plot(X_test[:,0], X_test[:,1], '*', markersize=10)\n",
    "\n",
    "    plt.axis('square')\n",
    "    ax.legend(['Class 0','Class 1','test point'])\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=15);\n",
    "    return fig, ax\n",
    "\n",
    "def decision_boundary(model, fig, ax, levels = [0.1,0.5,0.9], labels = True):\n",
    "    # Code for producing the plot\n",
    "    X1 = np.linspace(*ax.get_xlim(),100)\n",
    "    X2 = np.linspace(*ax.get_ylim(), 100)\n",
    "    Z = np.zeros(X1.shape+X2.shape)\n",
    "\n",
    "    for i,x1 in enumerate(X1):\n",
    "        for j,x2 in enumerate(X2):\n",
    "            _, y_prob = model.predict(np.array([x1,x2]).reshape(1,-1))\n",
    "            Z[j,i] = y_prob[0,0]\n",
    "            \n",
    "    contours = ax.contour(X1,X2,Z, levels=levels)\n",
    "    if labels:\n",
    "        ax.clabel(contours, inline=1, fontsize=10)\n",
    "        \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-Nearest-Neighbour Classification (3 pts)\n",
    "A KNN classifier assigns a test instance to the majority class associated with its $K$ nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of two classes (circles and squares) and a single test instance (star). Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "**Run the code in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_data(X_train, y_train, X_test)\n",
    "\n",
    "# Circles added to help visualize distance to points.\n",
    "for r in np.arange(0.25,1,0.25):\n",
    "    c = plt.Circle(X_test[0], r, color='g', fill=False)\n",
    "    ax.add_artist(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exercise 1 (1 pts)\n",
    "a) What would be the class assigned to this test instance for $K=2$, $K=4$, $K=5$ and why? (**0.5 pt**)\n",
    "\n",
    "b) The classification result is affected by the increasing $K$. What will be the maxinum value of K in this case? Why? (**0.5 pt**)<br />\n",
    "(***Hint:*** After $K$ reaches a certain value, it is impossible for the classification results to change. Find this value!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "In this task, we are going to complete a custom implementation of the unweighted K-Nearest-Neighbor classifier. A skeleton of the implementation can be found below, and the only function that is not finished is `predict(self, X_test)`.\n",
    "\n",
    "In total, the algorithm stores three variables:\n",
    "* `self.X_train`, `self.y_train`: All training data with labels.\n",
    "* `self.n_neighbors`: Number of neighbors, i.e. $K$.\n",
    "\n",
    "Your task is to finish the implementation of `predict(self, X_test)`. The distances between all test- and training instances have already been computed. In the loop below, the variable `dist_i` is an array containing the distances between test sample $i$ and all training points in `self.X_train`. Your task is to compute `y_pred_prob[i]`, which is an array with the probability for selecting each class in the dataset, e.g. for binary prediction we have $[1,0]$ (class 0), $[0,1]$ (class 1), or $[0.5,0.5]$ (tie).\n",
    "\n",
    "Finish this implementation and verify your results in Exercise 1 by running the implementation for the aforementioned values of $K$ (2,4, and 5).\n",
    "<br />\n",
    "***Hint:*** `np.unique` might be of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "class K_Nearest_Neighbors(): \n",
    "    \"\"\"\n",
    "    Custom implementation of K-nearest-neighbors.\n",
    "    \n",
    "    Note: If two or more classes have the same amount of neighbors, the prediction \n",
    "    will be random, i.e. tiebreaks are resolved randomly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neighbors):\n",
    "        \"\"\"\n",
    "        Parameter(s):\n",
    "            n_neighbors - Number of neighbors\n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        No training required. Store data and labels.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train   - Data of size (n_samples, n_features)\n",
    "            y_train   - True labels of size (n_samples,1)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Prediction of test data.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_test   - Data of size (n_samples, n_features)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "            y_pred - Predicted labels of size (n_samples,)\n",
    "            y_pred_prob - Probabilistic labels of size (n_samples,n_classes)\n",
    "        \"\"\"\n",
    "        n_samples = X_test.shape[0]\n",
    "        unique_classes = np.unique(self.y_train)\n",
    "        n_classes = len(unique_classes)\n",
    "\n",
    "        y_pred = np.zeros(shape=(n_samples,), dtype=np.int32)\n",
    "        y_pred_prob = np.zeros(shape=(n_samples, n_classes), dtype=np.float32)\n",
    "        \n",
    "        # Computes distances between all points in X_test and X_train.\n",
    "        dist = euclidean_distances(X_test,self.X_train)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            dist_i = dist[i]\n",
    "            \n",
    "            ### TODO - Change this part\n",
    "            y_pred_prob[i,:] = [1,0] # Always predicts class 0 currently\n",
    "            ### \n",
    "            \n",
    "            # Selects the prediction randomly, based on y_pred_prob[i]. \n",
    "            y_pred[i] = np.random.choice(unique_classes, p=y_pred_prob[i])\n",
    "        \n",
    "        return y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the code below after finishing the above implementation (no need to change anything).\n",
    "Make sure that you understand the plot and that your implementation produces sensible results.\n",
    "\n",
    "*Note:* What happens when $K$ is even?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (feel free to change)\n",
    "K = 3\n",
    "\n",
    "# Define and train the model\n",
    "model = K_Nearest_Neighbors(n_neighbors = K)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train,_ = model.predict(X_train)\n",
    "print(\"Train accuracy: %.3f %%\" %(np.mean(y_pred_train == y_train)*100))\n",
    "y_pred_test,_ = model.predict(X_test)\n",
    "print(\"Predicted test class, K=%d: %d\" %(K, y_pred_test))\n",
    "\n",
    "# Plot decision-boundaries\n",
    "fig, ax = plot_data(X_train, y_train, X_test, title = 'Lines showing decision-boundaries')\n",
    "fig, ax = decision_boundary(model, fig, ax, levels=[0.4, 0.6], labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "The Bayes classifier refers to a classifier using the predicitive probability:\n",
    "$$\n",
    "P(y_{n} = c | x_{n}, X, y) = \\frac{p(x_{n}|y_{n} = c, X, y)P(y_{n} = c | X, y)}{p(x_{n}|X,y)}, \\tag{1}\n",
    "$$\n",
    "where $X,y$ is the training data, $x_{n}$ a new test instance and $c$ its corresponding class.\n",
    "\n",
    "Assume that the class-conditional densities are Gaussian, that is\n",
    "$$\n",
    "    p(x_{n}|y_{n} = c, X, y) = \\mathcal{N}(\\mu_{c}, \\Sigma_{c}).\n",
    "$$\n",
    "\n",
    "Furthermore, assume a class size prior, i.e. the prior is set using the proportion of training points in class $c$:\n",
    "$$\n",
    "    P(y_{n} = c |X, y ) = \\frac{N_c}{N}, \\tag{2}\n",
    "$$\n",
    "where $N_c$ is the number of training objects belonging to class $c$, and $N$ is the total number of training objects.\n",
    "\n",
    "\n",
    "### Exercise  2.1 (3 pts)\n",
    "We consider a **binary** classification problem with **real valued data**, i.e. $x_n = [x_{n1}, x_{n2}]^T \\in \\mathbb{R}^2$ and $c=\\{0,1\\}$.\n",
    "\n",
    "\n",
    "#### 1.  (1 pts)\n",
    "In order to simplify the problem, we use the Naive Bayes assumption,\n",
    "$$\n",
    "    p(x_{n}|y_{n} = c, X, y) = \\prod_{d=1}^2 p(x_{nd}|y_n = c, X, y) = \\prod_{d=1}^2 \\mathcal{N}(\\mu_{cd}, \\sigma_{cd}^2),\n",
    "$$\n",
    "which assumes that the two attributes of $x_n$ are independent of one another, i.e. the covariance matrix $\\Sigma_{c}$ is diagonal.\n",
    "\n",
    "Using this assumption, write the expression for the **naive Bayes** classifier, that is, derive\n",
    "$$\n",
    "P(y_{n} = c | x_{n} , X, y ).\n",
    "$$\n",
    "\n",
    "***HINT:*** Derive the maximum likelihood estimates for the parameters $\\mu_{c}, \\Sigma_{c}$. Then express eq. 1 in terms of those estimates. \n",
    "\n",
    "#### 2. (2 pts)\n",
    "Derive the maximum likelihood estimate for $\\mu_{c}, \\Sigma_{c}$ when the covariance matrix is not diagonal, i.e, $\\Sigma_{c}$ has 4 unknown scalars. This alleviates the \"naive\" assumption, since the feature components are no longer independent of one another.\n",
    "\n",
    "***HINT:***\n",
    "When deriving the MLE for $\\Sigma_{c}$ it might be a good idea to consider $\\partial /\\partial \\Sigma_{c}^{-1}$. Also, feel free to use the following properties:\n",
    "$$\n",
    "    \\frac{\\partial\\ x^T A x}{\\partial A} = x x^T, \\qquad\n",
    "    \\frac{\\partial\\ x^T A x}{\\partial x} = 2Ax, \\qquad\n",
    "    \\frac{\\partial\\ \\log |A|}{\\partial A} = A^{-T},\n",
    "$$\n",
    "for a symmetric matrix $A$ and a vector $x$ that does not depend on $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  2.2 (1 pts)\n",
    "Here, you will finish a custom implementation of a Gaussian Naive Bayes classifier.\n",
    "\n",
    "The most important variables stored by the algorithm are:\n",
    "* `self.mean`, and `self.std`: Dictionaries containing MLE for each class. (`self.mean[c]`$ =[\\mu_{c1}, \\mu_{c2}]$, and `self.std[c]`$ =[\\sigma_{c1}, \\sigma_{c2}]$)\n",
    "* `self.prior`: Dictionary containing prior probability of each class. \n",
    "\n",
    "Use your calculations from Exercise 2.1.1 and complete the unfinished function `predict(self, X_test)`. The 1-D Gaussian density function `gaussian_density(self, x, mu, std)` is already implemented and may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes(): \n",
    "    \"\"\"\n",
    "    Custom implementation of a Gaussian Naive Bayes classifier.\n",
    "    The parameters are estimated using MLE.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Parameter(s):\n",
    "        \"\"\"\n",
    "        self.mean = {}\n",
    "        self.std = {}\n",
    "        self.prior = {}\n",
    "        self.unique_classes = 0\n",
    "        \n",
    "    def get_class_parameters(self, X_class):\n",
    "        \"\"\"\n",
    "        Estimating the MLE of the parameters.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_class - Data points corresponding to a single class\n",
    "        --------------------\n",
    "        Output(s):\n",
    "            mean_MLE - MLE of mean\n",
    "            std_MLE  - MLE of scale\n",
    "        \"\"\"\n",
    "        \n",
    "        mean_MLE = np.mean(X_class, axis=0)\n",
    "        std_MLE = np.std(X_class, axis=0)\n",
    "        \n",
    "        return mean_MLE, std_MLE\n",
    "            \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Compute model parameters using maximum likelihood estimates and a class size prior.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train   - Data of size (n_samples, n_features)\n",
    "            y_train   - True labels of size (n_samples,1)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute mean, variance and prior of each class\n",
    "        self.unique_classes = np.unique(y_train)\n",
    "        for uc in self.unique_classes:\n",
    "            X_class = X_train[y_train == uc]\n",
    "            c_mean, c_std = self.get_class_parameters(X_class)\n",
    "            self.mean[uc] = c_mean\n",
    "            self.std[uc] = c_std\n",
    "            self.prior[uc] = X_class.shape[0]/X_train.shape[0]\n",
    "            \n",
    "    def gaussian_density(self, x, mu, std):\n",
    "        \"\"\"\n",
    "        1-D Gaussian density function.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            x   - Data point\n",
    "            mu  - mean\n",
    "            std - scale\n",
    "        --------------------\n",
    "        Output(s):- \n",
    "            N(mu, std^2)\n",
    "\n",
    "        \"\"\"\n",
    "        return 1/(std*np.sqrt(2*np.pi))*np.exp(-(1/2)*((x-mu)/std)**2)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Prediction of test data.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_test   - Data of size (n_samples, n_features)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "            y_pred - Predicted labels of size (n_samples,)\n",
    "            y_pred_prob - Probabilistic labels of size (n_samples,n_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = X_test.shape[0]\n",
    "        y_pred_prob = np.zeros(shape=(n_samples,len(self.unique_classes)), dtype=np.float32)\n",
    "        \n",
    "        ### TODO - Change this part\n",
    "        for i in range(n_samples):\n",
    "            y_pred_prob[i,:] = [1,0] # Always predicts class 0 currently\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        y_pred = np.argmax(y_pred_prob, axis=-1)\n",
    "        return y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the code below after finishing the above implementation (no need to change anything).\n",
    "Make sure that you understand the plot and that your implementation produces sensible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the model\n",
    "model = GaussianNaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_train, _ = model.predict(X_train)\n",
    "print(\"Train accuracy: %.3f %%\" %(np.mean(y_pred_train == y_train)*100))\n",
    "y_pred_test,y_pred_prob_test = model.predict(X_test)\n",
    "print(\"Predicted test class: %d, (prob %.2f)\" %(y_pred_test, y_pred_prob_test[0,y_pred_test]))\n",
    "\n",
    "# Plot decision-boundaries\n",
    "fig, ax = plot_data(X_train, y_train, X_test, title = 'Lines showing $P(y_{n} = 0 | x_{n}, X, y)$')\n",
    "fig, ax = decision_boundary(model, fig, ax, labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM (4 pts)\n",
    "\n",
    "\n",
    "### Excercise 3.1 (2 pts)\n",
    "The SVM classifier is most often phrased as the constrained optimization problem:\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\underset{w,b}{\\arg\\min}\\ & \\frac{1}{2} w^T w \\\\\n",
    " s.t. \\quad & t_n(w^T x_n + b) \\geq 1, \\forall n,\n",
    " \\end{aligned}\n",
    "$$\n",
    "where $t_n \\in \\{-1,1\\}$ are now the associated labels ($y_n = 0 \\iff t_n = -1$; $y_n = 1 \\iff t_n = 1$).\n",
    "\n",
    "By moving the constraints into the objective function, we may formulate the problem using the *hinge* loss function:\n",
    "$$\n",
    "    J =  \\frac{1}{2} w^T w + C\\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - t_n(w^T x_n + b)),\n",
    "$$\n",
    "with $C$ as a regularization parameter.\n",
    "\n",
    "Even though the *hinge* loss function is not differentiable, it is still a convex function and has subgradients with respect to both $w$ and $b$. Thus, it can still be used for optimization.\n",
    "Derive the following subgradients:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}, \\quad\n",
    "\\frac{\\partial J}{\\partial b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3.2 (2 pts)\n",
    "Here, you will finish a custom implementation of a SVM classifier.\n",
    "\n",
    "The most important variables stored by the algorithm are:\n",
    "* `self.C`: Regularization parameter.\n",
    "* `self.parameters`: Dictionary containing the model parameters $w$ and $b$.\n",
    "\n",
    "Using the subderivatives derived in Exercise 3.1, complete the unfinished function `grad_cost(self, X_train, t, w, b)`, which should return the cost functions and the derivatives of the cost function w.r.t $w,b$.\n",
    "\n",
    "Also, finish the function `predict(self, X_test)` using the appropriate prediction strategy for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SVM(): \n",
    "    \"\"\"\n",
    "    Custom implementation of linear SVM.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, n_iterations, C = 1, print_cost=False):\n",
    "        \"\"\"\n",
    "        Parameter(s):\n",
    "            learning_rate - Learning rate\n",
    "            n_iterations  - Number of iterations\n",
    "            C             - Regularization parameter\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations \n",
    "        self.C = C\n",
    "        self.print_cost = print_cost\n",
    "        self.parameters = {}\n",
    "    \n",
    "    def initialize_parameters(self, n_features): \n",
    "        \"\"\"\n",
    "        Initialize model parameters with zeros:\n",
    "            w.shape = (n_features, 1)\n",
    "            b.shape = (1,)\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            n_features - Number of features \n",
    "        --------------------\n",
    "        Output(s):\n",
    "        \"\"\"\n",
    "\n",
    "        w = np.zeros(shape = (n_features,1))\n",
    "        b = 0\n",
    "\n",
    "        self.parameters[\"w\"] = w\n",
    "        self.parameters[\"b\"] = b\n",
    "\n",
    "    \n",
    "    def grad_cost(self, X_train, t, w, b):\n",
    "        \"\"\"\n",
    "        Computes the cost function (hinge loss) and \n",
    "        partial derivatives of cost w.r.t the model parameters.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train - Data of size(n_samples, n_features)\n",
    "            t       - Labels (-1 or 1) of size (n_samples,)\n",
    "            w,b     - Model parameters\n",
    "        --------------------\n",
    "        Output(s):\n",
    "            cost  - hinge loss\n",
    "            grads - Gradients of loss function w.r.t model parameters (dw,db). \n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        ### TODO - Change this part\n",
    "        # Dummy variables currently\n",
    "        cost = 0\n",
    "        dw = 0\n",
    "        db = 0\n",
    "        ### \n",
    "        \n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "\n",
    "        return cost, grads\n",
    "        \n",
    "    def fit(self, X_train, y_train): \n",
    "        \"\"\"\n",
    "        Optimize model parameters by running a gradient descent algorithm.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train - Data of size (n_samples, n_features)\n",
    "            y_train - True labels of size (n_samples,1)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "        \"\"\"\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        # Change the binary targets from (0,1) to (-1,1)\n",
    "        t = np.where(y_train < 0.5, -1, 1)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.params = self.initialize_parameters(n_features)\n",
    "        \n",
    "        # Gradient descent\n",
    "        w = self.parameters[\"w\"] \n",
    "        b = self.parameters[\"b\"] \n",
    "        for i in range(1,self.n_iterations+1):\n",
    "            cost, grads = self.grad_cost(X_train, t, w, b)\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "\n",
    "            w -= self.learning_rate * dw\n",
    "            b -= self.learning_rate * db \n",
    "            \n",
    "            if self.print_cost and i % 100 == 0:\n",
    "                print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "        self.parameters = {\"w\": w,\n",
    "                           \"b\": b}   \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Binary prediction of test data.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X   - Data of size (n_samples, n_features)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "            y_pred - Predicted labels (0/1) of size (n_samples,1)\n",
    "        \"\"\"\n",
    "        \n",
    "        w = self.parameters[\"w\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        \n",
    "        n_samples = X_test.shape[0]\n",
    "        y_pred = np.zeros((n_samples,1))\n",
    "        \n",
    "        ### TODO - Change this part\n",
    "        for i in range(n_samples):\n",
    "            y_pred[i] = -1 # Always predicts class 0 currently\n",
    "        ###    \n",
    "        \n",
    "        # Converts the binary targets from (-1,1) to (0,1)\n",
    "        y_pred = (y_pred > 0).astype(np.int32)\n",
    "        \n",
    "        return y_pred.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the code below after finishing the above implementation (no need to change anything).\n",
    "Make sure that you understand the plot and that your implementation produces sensible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameters (feel free to change)\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "C = 1\n",
    "print_cost = True\n",
    "\n",
    "# Define and train the model\n",
    "model = SVM(learning_rate, n_iterations, C, print_cost)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(\"Train accuracy: %.3f %%\" %(np.mean(y_pred_train == y_train)*100))\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(\"Predicted test class: %d\" %(y_pred_test))\n",
    "\n",
    "# Plot decision-boundaries\n",
    "fig, ax = plot_data(X_train, y_train, X_test, title = 'Line showing decision boundary')\n",
    "w = model.parameters['w']\n",
    "b = model.parameters['b']\n",
    "x = np.linspace(-3,3)\n",
    "ax.plot(x, (-b-w[0]*x)/w[1], '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "### Excercise 4.1 (2 pts)\n",
    "The last classifier that we will look at is Logistic Regression. Here you will finish a custom implementation of the Logistic Regression classifier using the following information:\n",
    "\n",
    "* The activation is computed as \n",
    "$$\n",
    "A = \\sigma(X w + b) = [a_1, \\ldots, a_n]^T,\n",
    "$$\n",
    "where $\\sigma(z)$ is the sigmoid-function.\n",
    "\n",
    "* The cost function is the negative log-likelihood:\n",
    "$$\n",
    "J = -\\frac{1}{n}\\sum_{i=1}^{n}y_i\\log(a_i)+(1-y_i)\\log(1-a_i).\n",
    "$$\n",
    "\n",
    "* Derivatives w.r.t the model parameters:\n",
    "$$\n",
    " \\frac{\\partial J}{\\partial w} = \\frac{1}{n}X^T(A-Y), \\qquad\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (a_i-y_i).\n",
    "$$\n",
    "\n",
    "Finish the implementation of `grad_cost(self, X_train, y_train, w, b)`, which once again should return the cost functions and the derivatives of the cost function w.r.t $w,b$.\n",
    "\n",
    "Also, finish the function `predict(self, X_test)` using the appropriate prediction strategy for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(): \n",
    "    \"\"\"\n",
    "    Custom implementation of (binary) Logistic Regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, n_iterations, print_cost=False):\n",
    "        \"\"\"\n",
    "        Parameter(s):\n",
    "            learning_rate - Learning rate\n",
    "            n_iterations  - Number of iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations \n",
    "        self.print_cost = print_cost\n",
    "        self.parameters = {}\n",
    "    \n",
    "    def initialize_parameters(self, n_features): \n",
    "        \"\"\"\n",
    "        Initialize model parameters with zeros:\n",
    "            w.shape = (n_features,)\n",
    "            b.shape = (1,)\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            n_features - Number of features \n",
    "        --------------------\n",
    "        Output(s):\n",
    "        \"\"\"\n",
    "        \n",
    "        w = np.zeros(shape = (n_features,1))\n",
    "        b = 0\n",
    "        \n",
    "        self.parameters['w'] = w\n",
    "        self.parameters['b'] = b\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            z - \n",
    "        --------------------\n",
    "        Output(s):\n",
    "            s - sigmoid(z)\n",
    "        \"\"\"\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "    \n",
    "    def grad_cost(self, X_train, y_train, w, b):\n",
    "        \"\"\"\n",
    "        Computes the cost function (negative log-likelihood) and \n",
    "        partial derivatives of cost w.r.t the model parameters.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train - Data of size(n_samples, n_features)\n",
    "            y_train - True labels of size (n_samples,1)\n",
    "            w,b     - Model parameters\n",
    "        --------------------\n",
    "        Output(s):\n",
    "            cost  - Negative log-likelihood cost\n",
    "            grads - Gradients of loss function w.r.t model parameters (dw,db). \n",
    "        \"\"\"\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        y_train = y_train.reshape(-1,1)\n",
    "        \n",
    "        ### TODO - Change this part\n",
    "        # Dummy variables currently\n",
    "        A = 0\n",
    "        cost = 0\n",
    "        dw = 0\n",
    "        db = 0\n",
    "        ###\n",
    "\n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "        \n",
    "        return cost, grads\n",
    "        \n",
    "    def fit(self, X_train, y_train): \n",
    "        \"\"\"\n",
    "        Optimize model parameters by running a gradient descent algorithm.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X_train - Data of size (n_samples, n_features)\n",
    "            y_train - True labels of size (n_samples,1)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "        \"\"\"\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.params = self.initialize_parameters(n_features)\n",
    "        \n",
    "        # Gradient descent\n",
    "        w = self.parameters[\"w\"] \n",
    "        b = self.parameters[\"b\"] \n",
    "        for i in range(1,self.n_iterations+1):\n",
    "            cost, grads = self.grad_cost(X_train, y_train, w, b)\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "\n",
    "            w -= self.learning_rate * dw\n",
    "            b -= self.learning_rate * db \n",
    "            \n",
    "            if self.print_cost and i % 100 == 0:\n",
    "                print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "        self.parameters = {\"w\": w,\n",
    "                           \"b\": b}   \n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Binary prediction of test data.\n",
    "        --------------------\n",
    "        Input(s):\n",
    "            X   - Data of size (n_samples, n_features)\n",
    "        --------------------\n",
    "        Output(s)\n",
    "            y_pred - Predicted labels (0/1) of size (n_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        w = self.parameters[\"w\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        \n",
    "        n_samples = X_test.shape[0]\n",
    "        y_pred = np.zeros((n_samples,1))\n",
    "        \n",
    "        ### TODO - Change this part\n",
    "        A = np.zeros((n_samples,1))                \n",
    "        for i in range(n_samples):\n",
    "            y_pred[i] = 0 # Always predicts class 0 currently\n",
    "        ###\n",
    "        \n",
    "        return y_pred.squeeze(), A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the code below after finishing the above implementation (no need to change anything).\n",
    "Make sure that you understand the plot and that your implementation produces sensible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (feel free to change)\n",
    "learning_rate = 0.05\n",
    "n_iterations = 1000\n",
    "print_cost = True\n",
    "\n",
    "# Define and train the model\n",
    "model = LogisticRegression(learning_rate, n_iterations, print_cost)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train, _ = model.predict(X_train)\n",
    "print(\"Train accuracy: %.3f %%\" %(np.mean(y_pred_train == y_train)*100))\n",
    "y_pred_test,y_pred_prob_test = model.predict(X_test)\n",
    "print(\"Predicted test class: %d, (prob %.2f)\" %(y_pred_test, np.where(y_pred_prob_test < 0.5, 1-y_pred_prob_test, y_pred_prob_test)))\n",
    "\n",
    "# Plot decision-boundaries\n",
    "fig, ax = plot_data(X_train, y_train, X_test, title = 'Lines showing $P(y_{n} = 1 | x_{n}, X, y)$')\n",
    "fig, ax = decision_boundary(model, fig, ax, labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (0 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets apply all four of our implemented classifiers to a real world dataset about breast cancer. The [UCI ML Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) dataset contains characteristics of the cell nuclei, which can be used to predict whether a tumor is malignant or benign.\n",
    "\n",
    "First, lets import the dataset and split it into a train- and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "X = X[:,:10] # Only looking at the first 10 features (the mean values)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Training set:\\t', X_train.shape,  y_train.shape)\n",
    "print('Test set:\\t', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the parameters for all of our classifiers and train each one of them and report the average train- and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (feel free to change)\n",
    "learning_rate = 0.05\n",
    "n_iterations = 1000\n",
    "C = 10\n",
    "print_cost = False\n",
    "K = 5\n",
    "\n",
    "# Define all models\n",
    "models = {\"KNN\":K_Nearest_Neighbors(n_neighbors = K),\n",
    "          \"SVM\":SVM(learning_rate, n_iterations, C, print_cost),\n",
    "          \"LR\":LogisticRegression(learning_rate, n_iterations, print_cost),\n",
    "          \"GNB\":GaussianNaiveBayes()}\n",
    "\n",
    "# Train all models and report performance\n",
    "for name, model in models.items():\n",
    "    print(name)\n",
    "    model.fit(X_train, y_train)\n",
    "    if name == \"SVM\":\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred_train,_ = model.predict(X_train)\n",
    "        y_pred_test,_ = model.predict(X_test)   \n",
    "    print(\"\\tTrain accuracy: %.3f %%\" %(np.mean(y_pred_train == y_train)*100))\n",
    "    print(\"\\tTest accuracy: %.3f %%\" %(np.mean(y_pred_test == y_test)*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
